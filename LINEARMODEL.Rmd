---
title: "linearmodel"
author: "Emraida ortanez"
date: "11/4/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
dataF<- read.csv(file="http://onlinestatbook.com/case_studies_rvls/physical_strength/data.txt",sep="",header=TRUE  )      
``` 

```{r include=FALSE}
require(tidyverse)
require(tigerstats)
require(knitr)
```



##  Now with ggplot - first select the basic data

```{r}
basicNN <- ggplot(dataF,aes(y=SIMS,x=ARM))
```

## Now add in scatterplot

```{r}
basicNN + geom_point()+geom_smooth(method=lm)
```

```{r}
basicNN <- ggplot(dataF,aes(y=SIMS,x=GRIP))
```

## Now add in scatterplot

```{r}
basicNN + geom_point()+geom_smooth(method=lm)
``` 
  
The plot of SIMS VS GRIP + ARM looks like a plane in 3D space and we don't have the tools available right now to plot it   

```{r}
model.1 <- lm(SIMS~ARM,data=dataF)
summary.lm(model.1)
```
```{r}
new <- data.frame(ARM=88,GRIP=94)
predict.lm(model.1,new)
predict(model.1,new,interval="prediction")
```
Model 1 is predicting a value of 0.7063836 and the 95 confidence interval is somewhere in the -1.726209 and 3.138977 . 

```{r}
model.2 <- lm(SIMS~GRIP,data=dataF)
summary.lm(model.2)
```

The Adjusted R Squared in the model for SIMS ARM has a higher value than SIMS GRIP. That means the model for ARM is better than GRIP

```{r}
new <- data.frame(ARM=88,GRIP=94)
predict.lm(model.2,new)
predict(model.2,new,interval="prediction")
```
Model 2 is predicting a value of -0.5361543 and the 95 confidence interval is somewhere in the -3.107961 and 2.035652 . 

```{r}
model.3 <- lm(SIMS~GRIP+ARM,data=dataF)
summary.lm(model.3)
```
The adjusted R squared for this model is 0.5358 which is bigger than the adjusted R squared for Model 1 ans Model 2 which explains 
more of a variation than either of the other 2 model does and just by exploring the adjusted R- squared if we are to rate the models 
Model 2 is the worst one , Model 1 is the second best one and Model 3 is the best Model.  

```{r}
new <- data.frame(ARM=88,GRIP=94)
predict.lm(model.3,new)
predict(model.3,new,interval="prediction")
```
```{r}
anova(model.1,model.3)
```
The RSS of Model 1 is 217.88 and RSS of Model 2 is 188.43 which is about 30 points less than SIMS~ARM. This shows that the result is 
signficant because we are testing whether or not it would change the value of our assessment. The result shows a low P value so we   
we reject the null hypothesis that there is isn't any difference in how well the Model 1 explains ARM and how well Model 2 explains  
ARM. This means that model 3 is doing a much better job of predicting it and that's why we reject the null hypothesis and therefore 
conclude that this particular anova test shows model 3 is the better model.  